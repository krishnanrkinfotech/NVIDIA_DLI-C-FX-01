{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-atf3gekcgR"
   },
   "source": [
    "# Assessment 1: I can train and deploy a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7wkT17FkmU6"
   },
   "source": [
    "At this point, you've worked through a full deep learning workflow. You've loaded a dataset, trained a model, and deployed your model into a simple application. Validate your learning by attempting to replicate that workflow with a new problem.\n",
    "\n",
    "We've included a dataset which consists of two classes:  \n",
    "\n",
    "1) Face: Contains images which include the face of a whale  \n",
    "2) Not Face: Contains images which do not include the face of a whale.  \n",
    "\n",
    "The dataset is located at ```/dli/data/whale/data/train```.\n",
    "\n",
    "Your challenge is:\n",
    "\n",
    "1) Use [DIGITS](/digits) to train a model to identify *new* whale faces with an accuracy of more than 80%.   \n",
    "\n",
    "2) Deploy your model by modifying and saving the python application [submission.py](../../../../edit/tasks/task-assessment/task/submission.py) to return the word \"whale\" if the image contains a whale's face and \"not whale\" if the image does not.  \n",
    "\n",
    "Resources:\n",
    "\n",
    "1) [Train a model](../../task1/task/Train%20a%20Model.ipynb)  \n",
    "2) [New Data as a goal](../../task2/task/New%20Data%20as%20a%20Goal.ipynb)  \n",
    "3) [Deployment](../../task3/task/Deployment.ipynb)  \n",
    "\n",
    "Suggestions: \n",
    "\n",
    "- Use empty code blocks to find out any informantion necessary to solve this problem: eg: ```!ls [directorypath] prints the files in a given directory``` \n",
    "- Executing the first two cells below will run your python script with test images, the first should return \"whale\" and the second should return \"not whale\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaaY1Vb3o3mC"
   },
   "source": [
    "Start in [DIGITS](/digits/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0707 15:06:00.131767   239 gpu_memory.cpp:105] GPUMemory::Manager initialized\n",
      "I0707 15:06:00.132774   239 gpu_memory.cpp:107] Total memory: 11996954624, Free: 11802116096, dev_info[0]: total=11996954624 free=11802116096\n",
      "W0707 15:06:00.132841   239 _caffe.cpp:172] DEPRECATION WARNING - deprecated use of Python interface\n",
      "W0707 15:06:00.132963   239 _caffe.cpp:173] Use this instead (with the named \"weights\" parameter):\n",
      "W0707 15:06:00.132980   239 _caffe.cpp:175] Net('/dli/data/digits/20200707-142333-5278/deploy.prototxt', 1, weights='/dli/data/digits/20200707-142333-5278/snapshot_iter_54.caffemodel')\n",
      "I0707 15:06:00.133299   239 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: /dli/data/digits/20200707-142333-5278/deploy.prototxt\n",
      "I0707 15:06:00.133328   239 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.\n",
      "W0707 15:06:00.133340   239 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.\n",
      "I0707 15:06:00.142812   239 net.cpp:79] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"input\"\n",
      "  type: \"Input\"\n",
      "  top: \"data\"\n",
      "  input_param {\n",
      "    shape {\n",
      "      dim: 1\n",
      "      dim: 3\n",
      "      dim: 227\n",
      "      dim: 227\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 96\n",
      "    kernel_size: 11\n",
      "    stride: 4\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"conv1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv4\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv4\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu4\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv4\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv5\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv5\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu5\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"conv5\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool5\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"pool5\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc6\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool5\"\n",
      "  top: \"fc6\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu6\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop6\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc7\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc7\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu7\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop7\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc8\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc8\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"softmax\"\n",
      "  type: \"Softmax\"\n",
      "  bottom: \"fc8\"\n",
      "  top: \"softmax\"\n",
      "}\n",
      "I0707 15:06:00.143257   239 net.cpp:109] Using FLOAT as default forward math type\n",
      "I0707 15:06:00.143277   239 net.cpp:115] Using FLOAT as default backward math type\n",
      "I0707 15:06:00.143290   239 layer_factory.hpp:172] Creating layer 'input' of type 'Input'\n",
      "I0707 15:06:00.143308   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.143330   239 net.cpp:199] Created Layer input (0)\n",
      "I0707 15:06:00.143349   239 net.cpp:541] input -> data\n",
      "I0707 15:06:00.144047   239 net.cpp:259] Setting up input\n",
      "I0707 15:06:00.144078   239 net.cpp:266] TEST Top shape for layer 0 'input' 1 3 227 227 (154587)\n",
      "I0707 15:06:00.144098   239 layer_factory.hpp:172] Creating layer 'conv1' of type 'Convolution'\n",
      "I0707 15:06:00.144114   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.144150   239 net.cpp:199] Created Layer conv1 (1)\n",
      "I0707 15:06:00.144162   239 net.cpp:571] conv1 <- data\n",
      "I0707 15:06:00.144176   239 net.cpp:541] conv1 -> conv1\n",
      "I0707 15:06:00.659121   239 net.cpp:259] Setting up conv1\n",
      "I0707 15:06:00.659166   239 net.cpp:266] TEST Top shape for layer 1 'conv1' 1 96 55 55 (290400)\n",
      "I0707 15:06:00.659193   239 layer_factory.hpp:172] Creating layer 'relu1' of type 'ReLU'\n",
      "I0707 15:06:00.659210   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.659229   239 net.cpp:199] Created Layer relu1 (2)\n",
      "I0707 15:06:00.659241   239 net.cpp:571] relu1 <- conv1\n",
      "I0707 15:06:00.659255   239 net.cpp:526] relu1 -> conv1 (in-place)\n",
      "I0707 15:06:00.659282   239 net.cpp:259] Setting up relu1\n",
      "I0707 15:06:00.659294   239 net.cpp:266] TEST Top shape for layer 2 'relu1' 1 96 55 55 (290400)\n",
      "I0707 15:06:00.659307   239 layer_factory.hpp:172] Creating layer 'norm1' of type 'LRN'\n",
      "I0707 15:06:00.659317   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.659340   239 net.cpp:199] Created Layer norm1 (3)\n",
      "I0707 15:06:00.659351   239 net.cpp:571] norm1 <- conv1\n",
      "I0707 15:06:00.659366   239 net.cpp:541] norm1 -> norm1\n",
      "I0707 15:06:00.659430   239 net.cpp:259] Setting up norm1\n",
      "I0707 15:06:00.659447   239 net.cpp:266] TEST Top shape for layer 3 'norm1' 1 96 55 55 (290400)\n",
      "I0707 15:06:00.659469   239 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'\n",
      "I0707 15:06:00.659485   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.659529   239 net.cpp:199] Created Layer pool1 (4)\n",
      "I0707 15:06:00.659541   239 net.cpp:571] pool1 <- norm1\n",
      "I0707 15:06:00.659552   239 net.cpp:541] pool1 -> pool1\n",
      "I0707 15:06:00.659607   239 net.cpp:259] Setting up pool1\n",
      "I0707 15:06:00.659622   239 net.cpp:266] TEST Top shape for layer 4 'pool1' 1 96 27 27 (69984)\n",
      "I0707 15:06:00.659634   239 layer_factory.hpp:172] Creating layer 'conv2' of type 'Convolution'\n",
      "I0707 15:06:00.659646   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.659667   239 net.cpp:199] Created Layer conv2 (5)\n",
      "I0707 15:06:00.659678   239 net.cpp:571] conv2 <- pool1\n",
      "I0707 15:06:00.659698   239 net.cpp:541] conv2 -> conv2\n",
      "I0707 15:06:00.666116   239 net.cpp:259] Setting up conv2\n",
      "I0707 15:06:00.666147   239 net.cpp:266] TEST Top shape for layer 5 'conv2' 1 256 27 27 (186624)\n",
      "I0707 15:06:00.666168   239 layer_factory.hpp:172] Creating layer 'relu2' of type 'ReLU'\n",
      "I0707 15:06:00.666188   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.666203   239 net.cpp:199] Created Layer relu2 (6)\n",
      "I0707 15:06:00.666218   239 net.cpp:571] relu2 <- conv2\n",
      "I0707 15:06:00.666236   239 net.cpp:526] relu2 -> conv2 (in-place)\n",
      "I0707 15:06:00.666258   239 net.cpp:259] Setting up relu2\n",
      "I0707 15:06:00.666276   239 net.cpp:266] TEST Top shape for layer 6 'relu2' 1 256 27 27 (186624)\n",
      "I0707 15:06:00.666288   239 layer_factory.hpp:172] Creating layer 'norm2' of type 'LRN'\n",
      "I0707 15:06:00.666303   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.666326   239 net.cpp:199] Created Layer norm2 (7)\n",
      "I0707 15:06:00.666339   239 net.cpp:571] norm2 <- conv2\n",
      "I0707 15:06:00.666349   239 net.cpp:541] norm2 -> norm2\n",
      "I0707 15:06:00.666404   239 net.cpp:259] Setting up norm2\n",
      "I0707 15:06:00.666424   239 net.cpp:266] TEST Top shape for layer 7 'norm2' 1 256 27 27 (186624)\n",
      "I0707 15:06:00.666435   239 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'\n",
      "I0707 15:06:00.666451   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.666471   239 net.cpp:199] Created Layer pool2 (8)\n",
      "I0707 15:06:00.666486   239 net.cpp:571] pool2 <- norm2\n",
      "I0707 15:06:00.666501   239 net.cpp:541] pool2 -> pool2\n",
      "I0707 15:06:00.666553   239 net.cpp:259] Setting up pool2\n",
      "I0707 15:06:00.666568   239 net.cpp:266] TEST Top shape for layer 8 'pool2' 1 256 13 13 (43264)\n",
      "I0707 15:06:00.666580   239 layer_factory.hpp:172] Creating layer 'conv3' of type 'Convolution'\n",
      "I0707 15:06:00.666592   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.666611   239 net.cpp:199] Created Layer conv3 (9)\n",
      "I0707 15:06:00.666622   239 net.cpp:571] conv3 <- pool2\n",
      "I0707 15:06:00.666635   239 net.cpp:541] conv3 -> conv3\n",
      "I0707 15:06:00.682404   239 net.cpp:259] Setting up conv3\n",
      "I0707 15:06:00.682448   239 net.cpp:266] TEST Top shape for layer 9 'conv3' 1 384 13 13 (64896)\n",
      "I0707 15:06:00.682480   239 layer_factory.hpp:172] Creating layer 'relu3' of type 'ReLU'\n",
      "I0707 15:06:00.682497   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.682520   239 net.cpp:199] Created Layer relu3 (10)\n",
      "I0707 15:06:00.682538   239 net.cpp:571] relu3 <- conv3\n",
      "I0707 15:06:00.682559   239 net.cpp:526] relu3 -> conv3 (in-place)\n",
      "I0707 15:06:00.682582   239 net.cpp:259] Setting up relu3\n",
      "I0707 15:06:00.682598   239 net.cpp:266] TEST Top shape for layer 10 'relu3' 1 384 13 13 (64896)\n",
      "I0707 15:06:00.682610   239 layer_factory.hpp:172] Creating layer 'conv4' of type 'Convolution'\n",
      "I0707 15:06:00.682626   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.682654   239 net.cpp:199] Created Layer conv4 (11)\n",
      "I0707 15:06:00.682667   239 net.cpp:571] conv4 <- conv3\n",
      "I0707 15:06:00.682683   239 net.cpp:541] conv4 -> conv4\n",
      "I0707 15:06:00.695369   239 net.cpp:259] Setting up conv4\n",
      "I0707 15:06:00.695405   239 net.cpp:266] TEST Top shape for layer 11 'conv4' 1 384 13 13 (64896)\n",
      "I0707 15:06:00.695452   239 layer_factory.hpp:172] Creating layer 'relu4' of type 'ReLU'\n",
      "I0707 15:06:00.695466   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.695482   239 net.cpp:199] Created Layer relu4 (12)\n",
      "I0707 15:06:00.695494   239 net.cpp:571] relu4 <- conv4\n",
      "I0707 15:06:00.695506   239 net.cpp:526] relu4 -> conv4 (in-place)\n",
      "I0707 15:06:00.695523   239 net.cpp:259] Setting up relu4\n",
      "I0707 15:06:00.695534   239 net.cpp:266] TEST Top shape for layer 12 'relu4' 1 384 13 13 (64896)\n",
      "I0707 15:06:00.695546   239 layer_factory.hpp:172] Creating layer 'conv5' of type 'Convolution'\n",
      "I0707 15:06:00.695559   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.695578   239 net.cpp:199] Created Layer conv5 (13)\n",
      "I0707 15:06:00.695590   239 net.cpp:571] conv5 <- conv4\n",
      "I0707 15:06:00.695601   239 net.cpp:541] conv5 -> conv5\n",
      "I0707 15:06:00.703550   239 net.cpp:259] Setting up conv5\n",
      "I0707 15:06:00.703575   239 net.cpp:266] TEST Top shape for layer 13 'conv5' 1 256 13 13 (43264)\n",
      "I0707 15:06:00.703608   239 layer_factory.hpp:172] Creating layer 'relu5' of type 'ReLU'\n",
      "I0707 15:06:00.703626   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.703641   239 net.cpp:199] Created Layer relu5 (14)\n",
      "I0707 15:06:00.703656   239 net.cpp:571] relu5 <- conv5\n",
      "I0707 15:06:00.703668   239 net.cpp:526] relu5 -> conv5 (in-place)\n",
      "I0707 15:06:00.703689   239 net.cpp:259] Setting up relu5\n",
      "I0707 15:06:00.703707   239 net.cpp:266] TEST Top shape for layer 14 'relu5' 1 256 13 13 (43264)\n",
      "I0707 15:06:00.703724   239 layer_factory.hpp:172] Creating layer 'pool5' of type 'Pooling'\n",
      "I0707 15:06:00.703735   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.703759   239 net.cpp:199] Created Layer pool5 (15)\n",
      "I0707 15:06:00.703774   239 net.cpp:571] pool5 <- conv5\n",
      "I0707 15:06:00.703786   239 net.cpp:541] pool5 -> pool5\n",
      "I0707 15:06:00.703853   239 net.cpp:259] Setting up pool5\n",
      "I0707 15:06:00.703873   239 net.cpp:266] TEST Top shape for layer 15 'pool5' 1 256 6 6 (9216)\n",
      "I0707 15:06:00.703886   239 layer_factory.hpp:172] Creating layer 'fc6' of type 'InnerProduct'\n",
      "I0707 15:06:00.703902   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:00.703922   239 net.cpp:199] Created Layer fc6 (16)\n",
      "I0707 15:06:00.703935   239 net.cpp:571] fc6 <- pool5\n",
      "I0707 15:06:00.703953   239 net.cpp:541] fc6 -> fc6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0707 15:06:01.376785   239 net.cpp:259] Setting up fc6\n",
      "I0707 15:06:01.376829   239 net.cpp:266] TEST Top shape for layer 16 'fc6' 1 4096 (4096)\n",
      "I0707 15:06:01.376859   239 layer_factory.hpp:172] Creating layer 'relu6' of type 'ReLU'\n",
      "I0707 15:06:01.376875   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:01.376893   239 net.cpp:199] Created Layer relu6 (17)\n",
      "I0707 15:06:01.376909   239 net.cpp:571] relu6 <- fc6\n",
      "I0707 15:06:01.376926   239 net.cpp:526] relu6 -> fc6 (in-place)\n",
      "I0707 15:06:01.376952   239 net.cpp:259] Setting up relu6\n",
      "I0707 15:06:01.376969   239 net.cpp:266] TEST Top shape for layer 17 'relu6' 1 4096 (4096)\n",
      "I0707 15:06:01.376981   239 layer_factory.hpp:172] Creating layer 'drop6' of type 'Dropout'\n",
      "I0707 15:06:01.376997   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:01.377017   239 net.cpp:199] Created Layer drop6 (18)\n",
      "I0707 15:06:01.377030   239 net.cpp:571] drop6 <- fc6\n",
      "I0707 15:06:01.377045   239 net.cpp:526] drop6 -> fc6 (in-place)\n",
      "I0707 15:06:01.410642   239 net.cpp:259] Setting up drop6\n",
      "I0707 15:06:01.410691   239 net.cpp:266] TEST Top shape for layer 18 'drop6' 1 4096 (4096)\n",
      "I0707 15:06:01.410712   239 layer_factory.hpp:172] Creating layer 'fc7' of type 'InnerProduct'\n",
      "I0707 15:06:01.410727   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:01.410748   239 net.cpp:199] Created Layer fc7 (19)\n",
      "I0707 15:06:01.410792   239 net.cpp:571] fc7 <- fc6\n",
      "I0707 15:06:01.410809   239 net.cpp:541] fc7 -> fc7\n",
      "I0707 15:06:01.711534   239 net.cpp:259] Setting up fc7\n",
      "I0707 15:06:01.711582   239 net.cpp:266] TEST Top shape for layer 19 'fc7' 1 4096 (4096)\n",
      "I0707 15:06:01.711611   239 layer_factory.hpp:172] Creating layer 'relu7' of type 'ReLU'\n",
      "I0707 15:06:01.711632   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:01.711650   239 net.cpp:199] Created Layer relu7 (20)\n",
      "I0707 15:06:01.711668   239 net.cpp:571] relu7 <- fc7\n",
      "I0707 15:06:01.711683   239 net.cpp:526] relu7 -> fc7 (in-place)\n",
      "I0707 15:06:01.711709   239 net.cpp:259] Setting up relu7\n",
      "I0707 15:06:01.711724   239 net.cpp:266] TEST Top shape for layer 20 'relu7' 1 4096 (4096)\n",
      "I0707 15:06:01.711741   239 layer_factory.hpp:172] Creating layer 'drop7' of type 'Dropout'\n",
      "I0707 15:06:01.711755   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:01.711772   239 net.cpp:199] Created Layer drop7 (21)\n",
      "I0707 15:06:01.711786   239 net.cpp:571] drop7 <- fc7\n",
      "I0707 15:06:01.711798   239 net.cpp:526] drop7 -> fc7 (in-place)\n",
      "I0707 15:06:01.745570   239 net.cpp:259] Setting up drop7\n",
      "I0707 15:06:01.745615   239 net.cpp:266] TEST Top shape for layer 21 'drop7' 1 4096 (4096)\n",
      "I0707 15:06:01.745638   239 layer_factory.hpp:172] Creating layer 'fc8' of type 'InnerProduct'\n",
      "I0707 15:06:01.745656   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:01.745684   239 net.cpp:199] Created Layer fc8 (22)\n",
      "I0707 15:06:01.745702   239 net.cpp:571] fc8 <- fc7\n",
      "I0707 15:06:01.745714   239 net.cpp:541] fc8 -> fc8\n",
      "I0707 15:06:01.746026   239 net.cpp:259] Setting up fc8\n",
      "I0707 15:06:01.746047   239 net.cpp:266] TEST Top shape for layer 22 'fc8' 1 2 (2)\n",
      "I0707 15:06:01.746065   239 layer_factory.hpp:172] Creating layer 'softmax' of type 'Softmax'\n",
      "I0707 15:06:01.746078   239 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:01.746102   239 net.cpp:199] Created Layer softmax (23)\n",
      "I0707 15:06:01.746119   239 net.cpp:571] softmax <- fc8\n",
      "I0707 15:06:01.746135   239 net.cpp:541] softmax -> softmax\n",
      "I0707 15:06:01.746225   239 net.cpp:259] Setting up softmax\n",
      "I0707 15:06:01.746251   239 net.cpp:266] TEST Top shape for layer 23 'softmax' 1 2 (2)\n",
      "I0707 15:06:01.746270   239 net.cpp:337] softmax does not need backward computation.\n",
      "I0707 15:06:01.746287   239 net.cpp:337] fc8 does not need backward computation.\n",
      "I0707 15:06:01.746300   239 net.cpp:337] drop7 does not need backward computation.\n",
      "I0707 15:06:01.746307   239 net.cpp:337] relu7 does not need backward computation.\n",
      "I0707 15:06:01.746317   239 net.cpp:337] fc7 does not need backward computation.\n",
      "I0707 15:06:01.746322   239 net.cpp:337] drop6 does not need backward computation.\n",
      "I0707 15:06:01.746332   239 net.cpp:337] relu6 does not need backward computation.\n",
      "I0707 15:06:01.746340   239 net.cpp:337] fc6 does not need backward computation.\n",
      "I0707 15:06:01.746349   239 net.cpp:337] pool5 does not need backward computation.\n",
      "I0707 15:06:01.746356   239 net.cpp:337] relu5 does not need backward computation.\n",
      "I0707 15:06:01.746366   239 net.cpp:337] conv5 does not need backward computation.\n",
      "I0707 15:06:01.746372   239 net.cpp:337] relu4 does not need backward computation.\n",
      "I0707 15:06:01.746381   239 net.cpp:337] conv4 does not need backward computation.\n",
      "I0707 15:06:01.746387   239 net.cpp:337] relu3 does not need backward computation.\n",
      "I0707 15:06:01.746397   239 net.cpp:337] conv3 does not need backward computation.\n",
      "I0707 15:06:01.746404   239 net.cpp:337] pool2 does not need backward computation.\n",
      "I0707 15:06:01.746414   239 net.cpp:337] norm2 does not need backward computation.\n",
      "I0707 15:06:01.746421   239 net.cpp:337] relu2 does not need backward computation.\n",
      "I0707 15:06:01.746430   239 net.cpp:337] conv2 does not need backward computation.\n",
      "I0707 15:06:01.746438   239 net.cpp:337] pool1 does not need backward computation.\n",
      "I0707 15:06:01.746481   239 net.cpp:337] norm1 does not need backward computation.\n",
      "I0707 15:06:01.746497   239 net.cpp:337] relu1 does not need backward computation.\n",
      "I0707 15:06:01.746510   239 net.cpp:337] conv1 does not need backward computation.\n",
      "I0707 15:06:01.746526   239 net.cpp:337] input does not need backward computation.\n",
      "I0707 15:06:01.746543   239 net.cpp:379] This network produces output softmax\n",
      "I0707 15:06:01.746583   239 net.cpp:402] Top memory (TEST) required for data: 8315264 diff: 8315264\n",
      "I0707 15:06:01.746596   239 net.cpp:405] Bottom memory (TEST) required for data: 8315256 diff: 8315256\n",
      "I0707 15:06:01.746603   239 net.cpp:408] Shared (in-place) memory (TEST) by data: 2665856 diff: 2665856\n",
      "I0707 15:06:01.746613   239 net.cpp:411] Parameters memory (TEST) required for data: 227505672 diff: 227505672\n",
      "I0707 15:06:01.746618   239 net.cpp:414] Parameters shared memory (TEST) by data: 0 diff: 0\n",
      "I0707 15:06:01.746628   239 net.cpp:420] Network initialization done.\n",
      "I0707 15:06:01.856899   239 net.cpp:1129] Ignoring source layer train-data\n",
      "I0707 15:06:01.856940   239 net.cpp:1137] Copying source layer conv1 Type:Convolution #blobs=2\n",
      "I0707 15:06:01.857061   239 net.cpp:1137] Copying source layer relu1 Type:ReLU #blobs=0\n",
      "I0707 15:06:01.857079   239 net.cpp:1137] Copying source layer norm1 Type:LRN #blobs=0\n",
      "I0707 15:06:01.857085   239 net.cpp:1137] Copying source layer pool1 Type:Pooling #blobs=0\n",
      "I0707 15:06:01.857095   239 net.cpp:1137] Copying source layer conv2 Type:Convolution #blobs=2\n",
      "I0707 15:06:01.857309   239 net.cpp:1137] Copying source layer relu2 Type:ReLU #blobs=0\n",
      "I0707 15:06:01.857328   239 net.cpp:1137] Copying source layer norm2 Type:LRN #blobs=0\n",
      "I0707 15:06:01.857339   239 net.cpp:1137] Copying source layer pool2 Type:Pooling #blobs=0\n",
      "I0707 15:06:01.857352   239 net.cpp:1137] Copying source layer conv3 Type:Convolution #blobs=2\n",
      "I0707 15:06:01.857831   239 net.cpp:1137] Copying source layer relu3 Type:ReLU #blobs=0\n",
      "I0707 15:06:01.857847   239 net.cpp:1137] Copying source layer conv4 Type:Convolution #blobs=2\n",
      "I0707 15:06:01.858212   239 net.cpp:1137] Copying source layer relu4 Type:ReLU #blobs=0\n",
      "I0707 15:06:01.858228   239 net.cpp:1137] Copying source layer conv5 Type:Convolution #blobs=2\n",
      "I0707 15:06:01.858464   239 net.cpp:1137] Copying source layer relu5 Type:ReLU #blobs=0\n",
      "I0707 15:06:01.858480   239 net.cpp:1137] Copying source layer pool5 Type:Pooling #blobs=0\n",
      "I0707 15:06:01.858496   239 net.cpp:1137] Copying source layer fc6 Type:InnerProduct #blobs=2\n",
      "I0707 15:06:01.875602   239 net.cpp:1137] Copying source layer relu6 Type:ReLU #blobs=0\n",
      "I0707 15:06:01.875634   239 net.cpp:1137] Copying source layer drop6 Type:Dropout #blobs=0\n",
      "I0707 15:06:01.875646   239 net.cpp:1137] Copying source layer fc7 Type:InnerProduct #blobs=2\n",
      "I0707 15:06:01.883476   239 net.cpp:1137] Copying source layer relu7 Type:ReLU #blobs=0\n",
      "I0707 15:06:01.883507   239 net.cpp:1137] Copying source layer drop7 Type:Dropout #blobs=0\n",
      "I0707 15:06:01.883512   239 net.cpp:1137] Copying source layer fc8 Type:InnerProduct #blobs=2\n",
      "I0707 15:06:01.883563   239 net.cpp:1129] Ignoring source layer loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whale\r\n"
     ]
    }
   ],
   "source": [
    "!python submission.py '/dli/data/whale/data/train/face/w_1.jpg'  #This should return \"whale\" at the very bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0707 15:06:39.996333   254 gpu_memory.cpp:105] GPUMemory::Manager initialized\n",
      "I0707 15:06:39.997330   254 gpu_memory.cpp:107] Total memory: 11996954624, Free: 11802116096, dev_info[0]: total=11996954624 free=11802116096\n",
      "W0707 15:06:39.997388   254 _caffe.cpp:172] DEPRECATION WARNING - deprecated use of Python interface\n",
      "W0707 15:06:39.997510   254 _caffe.cpp:173] Use this instead (with the named \"weights\" parameter):\n",
      "W0707 15:06:39.997525   254 _caffe.cpp:175] Net('/dli/data/digits/20200707-142333-5278/deploy.prototxt', 1, weights='/dli/data/digits/20200707-142333-5278/snapshot_iter_54.caffemodel')\n",
      "I0707 15:06:39.997833   254 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: /dli/data/digits/20200707-142333-5278/deploy.prototxt\n",
      "I0707 15:06:39.997862   254 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.\n",
      "W0707 15:06:39.997874   254 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.\n",
      "I0707 15:06:40.006798   254 net.cpp:79] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"input\"\n",
      "  type: \"Input\"\n",
      "  top: \"data\"\n",
      "  input_param {\n",
      "    shape {\n",
      "      dim: 1\n",
      "      dim: 3\n",
      "      dim: 227\n",
      "      dim: 227\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 96\n",
      "    kernel_size: 11\n",
      "    stride: 4\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"conv1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv4\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv4\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu4\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv4\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv5\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv5\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu5\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"conv5\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool5\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"pool5\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc6\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool5\"\n",
      "  top: \"fc6\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu6\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop6\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc7\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc7\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu7\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop7\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc8\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc8\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"softmax\"\n",
      "  type: \"Softmax\"\n",
      "  bottom: \"fc8\"\n",
      "  top: \"softmax\"\n",
      "}\n",
      "I0707 15:06:40.007323   254 net.cpp:109] Using FLOAT as default forward math type\n",
      "I0707 15:06:40.007339   254 net.cpp:115] Using FLOAT as default backward math type\n",
      "I0707 15:06:40.007352   254 layer_factory.hpp:172] Creating layer 'input' of type 'Input'\n",
      "I0707 15:06:40.007365   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.007381   254 net.cpp:199] Created Layer input (0)\n",
      "I0707 15:06:40.007396   254 net.cpp:541] input -> data\n",
      "I0707 15:06:40.008020   254 net.cpp:259] Setting up input\n",
      "I0707 15:06:40.008049   254 net.cpp:266] TEST Top shape for layer 0 'input' 1 3 227 227 (154587)\n",
      "I0707 15:06:40.008065   254 layer_factory.hpp:172] Creating layer 'conv1' of type 'Convolution'\n",
      "I0707 15:06:40.008078   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.008111   254 net.cpp:199] Created Layer conv1 (1)\n",
      "I0707 15:06:40.008124   254 net.cpp:571] conv1 <- data\n",
      "I0707 15:06:40.008137   254 net.cpp:541] conv1 -> conv1\n",
      "I0707 15:06:40.519101   254 net.cpp:259] Setting up conv1\n",
      "I0707 15:06:40.519146   254 net.cpp:266] TEST Top shape for layer 1 'conv1' 1 96 55 55 (290400)\n",
      "I0707 15:06:40.519170   254 layer_factory.hpp:172] Creating layer 'relu1' of type 'ReLU'\n",
      "I0707 15:06:40.519186   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.519201   254 net.cpp:199] Created Layer relu1 (2)\n",
      "I0707 15:06:40.519212   254 net.cpp:571] relu1 <- conv1\n",
      "I0707 15:06:40.519222   254 net.cpp:526] relu1 -> conv1 (in-place)\n",
      "I0707 15:06:40.519248   254 net.cpp:259] Setting up relu1\n",
      "I0707 15:06:40.519259   254 net.cpp:266] TEST Top shape for layer 2 'relu1' 1 96 55 55 (290400)\n",
      "I0707 15:06:40.519273   254 layer_factory.hpp:172] Creating layer 'norm1' of type 'LRN'\n",
      "I0707 15:06:40.519282   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.519305   254 net.cpp:199] Created Layer norm1 (3)\n",
      "I0707 15:06:40.519316   254 net.cpp:571] norm1 <- conv1\n",
      "I0707 15:06:40.519323   254 net.cpp:541] norm1 -> norm1\n",
      "I0707 15:06:40.519376   254 net.cpp:259] Setting up norm1\n",
      "I0707 15:06:40.519393   254 net.cpp:266] TEST Top shape for layer 3 'norm1' 1 96 55 55 (290400)\n",
      "I0707 15:06:40.519400   254 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'\n",
      "I0707 15:06:40.519412   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.519455   254 net.cpp:199] Created Layer pool1 (4)\n",
      "I0707 15:06:40.519464   254 net.cpp:571] pool1 <- norm1\n",
      "I0707 15:06:40.519474   254 net.cpp:541] pool1 -> pool1\n",
      "I0707 15:06:40.519529   254 net.cpp:259] Setting up pool1\n",
      "I0707 15:06:40.519544   254 net.cpp:266] TEST Top shape for layer 4 'pool1' 1 96 27 27 (69984)\n",
      "I0707 15:06:40.519556   254 layer_factory.hpp:172] Creating layer 'conv2' of type 'Convolution'\n",
      "I0707 15:06:40.519568   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.519592   254 net.cpp:199] Created Layer conv2 (5)\n",
      "I0707 15:06:40.519603   254 net.cpp:571] conv2 <- pool1\n",
      "I0707 15:06:40.519614   254 net.cpp:541] conv2 -> conv2\n",
      "I0707 15:06:40.526054   254 net.cpp:259] Setting up conv2\n",
      "I0707 15:06:40.526079   254 net.cpp:266] TEST Top shape for layer 5 'conv2' 1 256 27 27 (186624)\n",
      "I0707 15:06:40.526099   254 layer_factory.hpp:172] Creating layer 'relu2' of type 'ReLU'\n",
      "I0707 15:06:40.526113   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.526127   254 net.cpp:199] Created Layer relu2 (6)\n",
      "I0707 15:06:40.526139   254 net.cpp:571] relu2 <- conv2\n",
      "I0707 15:06:40.526151   254 net.cpp:526] relu2 -> conv2 (in-place)\n",
      "I0707 15:06:40.526166   254 net.cpp:259] Setting up relu2\n",
      "I0707 15:06:40.526178   254 net.cpp:266] TEST Top shape for layer 6 'relu2' 1 256 27 27 (186624)\n",
      "I0707 15:06:40.526190   254 layer_factory.hpp:172] Creating layer 'norm2' of type 'LRN'\n",
      "I0707 15:06:40.526201   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.526218   254 net.cpp:199] Created Layer norm2 (7)\n",
      "I0707 15:06:40.526228   254 net.cpp:571] norm2 <- conv2\n",
      "I0707 15:06:40.526240   254 net.cpp:541] norm2 -> norm2\n",
      "I0707 15:06:40.526286   254 net.cpp:259] Setting up norm2\n",
      "I0707 15:06:40.526301   254 net.cpp:266] TEST Top shape for layer 7 'norm2' 1 256 27 27 (186624)\n",
      "I0707 15:06:40.526312   254 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'\n",
      "I0707 15:06:40.526324   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.526335   254 net.cpp:199] Created Layer pool2 (8)\n",
      "I0707 15:06:40.526346   254 net.cpp:571] pool2 <- norm2\n",
      "I0707 15:06:40.526353   254 net.cpp:541] pool2 -> pool2\n",
      "I0707 15:06:40.526401   254 net.cpp:259] Setting up pool2\n",
      "I0707 15:06:40.526415   254 net.cpp:266] TEST Top shape for layer 8 'pool2' 1 256 13 13 (43264)\n",
      "I0707 15:06:40.526428   254 layer_factory.hpp:172] Creating layer 'conv3' of type 'Convolution'\n",
      "I0707 15:06:40.526439   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.526458   254 net.cpp:199] Created Layer conv3 (9)\n",
      "I0707 15:06:40.526469   254 net.cpp:571] conv3 <- pool2\n",
      "I0707 15:06:40.526481   254 net.cpp:541] conv3 -> conv3\n",
      "I0707 15:06:40.542059   254 net.cpp:259] Setting up conv3\n",
      "I0707 15:06:40.542104   254 net.cpp:266] TEST Top shape for layer 9 'conv3' 1 384 13 13 (64896)\n",
      "I0707 15:06:40.542124   254 layer_factory.hpp:172] Creating layer 'relu3' of type 'ReLU'\n",
      "I0707 15:06:40.542138   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.542157   254 net.cpp:199] Created Layer relu3 (10)\n",
      "I0707 15:06:40.542171   254 net.cpp:571] relu3 <- conv3\n",
      "I0707 15:06:40.542184   254 net.cpp:526] relu3 -> conv3 (in-place)\n",
      "I0707 15:06:40.542202   254 net.cpp:259] Setting up relu3\n",
      "I0707 15:06:40.542215   254 net.cpp:266] TEST Top shape for layer 10 'relu3' 1 384 13 13 (64896)\n",
      "I0707 15:06:40.542227   254 layer_factory.hpp:172] Creating layer 'conv4' of type 'Convolution'\n",
      "I0707 15:06:40.542238   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.542261   254 net.cpp:199] Created Layer conv4 (11)\n",
      "I0707 15:06:40.542273   254 net.cpp:571] conv4 <- conv3\n",
      "I0707 15:06:40.542284   254 net.cpp:541] conv4 -> conv4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0707 15:06:40.555265   254 net.cpp:259] Setting up conv4\n",
      "I0707 15:06:40.555313   254 net.cpp:266] TEST Top shape for layer 11 'conv4' 1 384 13 13 (64896)\n",
      "I0707 15:06:40.555371   254 layer_factory.hpp:172] Creating layer 'relu4' of type 'ReLU'\n",
      "I0707 15:06:40.555387   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.555406   254 net.cpp:199] Created Layer relu4 (12)\n",
      "I0707 15:06:40.555420   254 net.cpp:571] relu4 <- conv4\n",
      "I0707 15:06:40.555436   254 net.cpp:526] relu4 -> conv4 (in-place)\n",
      "I0707 15:06:40.555454   254 net.cpp:259] Setting up relu4\n",
      "I0707 15:06:40.555467   254 net.cpp:266] TEST Top shape for layer 12 'relu4' 1 384 13 13 (64896)\n",
      "I0707 15:06:40.555480   254 layer_factory.hpp:172] Creating layer 'conv5' of type 'Convolution'\n",
      "I0707 15:06:40.555492   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.555511   254 net.cpp:199] Created Layer conv5 (13)\n",
      "I0707 15:06:40.555522   254 net.cpp:571] conv5 <- conv4\n",
      "I0707 15:06:40.555531   254 net.cpp:541] conv5 -> conv5\n",
      "I0707 15:06:40.563621   254 net.cpp:259] Setting up conv5\n",
      "I0707 15:06:40.563648   254 net.cpp:266] TEST Top shape for layer 13 'conv5' 1 256 13 13 (43264)\n",
      "I0707 15:06:40.563668   254 layer_factory.hpp:172] Creating layer 'relu5' of type 'ReLU'\n",
      "I0707 15:06:40.563683   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.563691   254 net.cpp:199] Created Layer relu5 (14)\n",
      "I0707 15:06:40.563704   254 net.cpp:571] relu5 <- conv5\n",
      "I0707 15:06:40.563716   254 net.cpp:526] relu5 -> conv5 (in-place)\n",
      "I0707 15:06:40.563732   254 net.cpp:259] Setting up relu5\n",
      "I0707 15:06:40.563745   254 net.cpp:266] TEST Top shape for layer 14 'relu5' 1 256 13 13 (43264)\n",
      "I0707 15:06:40.563755   254 layer_factory.hpp:172] Creating layer 'pool5' of type 'Pooling'\n",
      "I0707 15:06:40.563766   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.563783   254 net.cpp:199] Created Layer pool5 (15)\n",
      "I0707 15:06:40.563794   254 net.cpp:571] pool5 <- conv5\n",
      "I0707 15:06:40.563807   254 net.cpp:541] pool5 -> pool5\n",
      "I0707 15:06:40.563868   254 net.cpp:259] Setting up pool5\n",
      "I0707 15:06:40.563884   254 net.cpp:266] TEST Top shape for layer 15 'pool5' 1 256 6 6 (9216)\n",
      "I0707 15:06:40.563897   254 layer_factory.hpp:172] Creating layer 'fc6' of type 'InnerProduct'\n",
      "I0707 15:06:40.563906   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:40.563925   254 net.cpp:199] Created Layer fc6 (16)\n",
      "I0707 15:06:40.563936   254 net.cpp:571] fc6 <- pool5\n",
      "I0707 15:06:40.563943   254 net.cpp:541] fc6 -> fc6\n",
      "I0707 15:06:41.234138   254 net.cpp:259] Setting up fc6\n",
      "I0707 15:06:41.234181   254 net.cpp:266] TEST Top shape for layer 16 'fc6' 1 4096 (4096)\n",
      "I0707 15:06:41.234202   254 layer_factory.hpp:172] Creating layer 'relu6' of type 'ReLU'\n",
      "I0707 15:06:41.234217   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:41.234230   254 net.cpp:199] Created Layer relu6 (17)\n",
      "I0707 15:06:41.234242   254 net.cpp:571] relu6 <- fc6\n",
      "I0707 15:06:41.234254   254 net.cpp:526] relu6 -> fc6 (in-place)\n",
      "I0707 15:06:41.234274   254 net.cpp:259] Setting up relu6\n",
      "I0707 15:06:41.234285   254 net.cpp:266] TEST Top shape for layer 17 'relu6' 1 4096 (4096)\n",
      "I0707 15:06:41.234292   254 layer_factory.hpp:172] Creating layer 'drop6' of type 'Dropout'\n",
      "I0707 15:06:41.234304   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:41.234316   254 net.cpp:199] Created Layer drop6 (18)\n",
      "I0707 15:06:41.234326   254 net.cpp:571] drop6 <- fc6\n",
      "I0707 15:06:41.234333   254 net.cpp:526] drop6 -> fc6 (in-place)\n",
      "I0707 15:06:41.267957   254 net.cpp:259] Setting up drop6\n",
      "I0707 15:06:41.268025   254 net.cpp:266] TEST Top shape for layer 18 'drop6' 1 4096 (4096)\n",
      "I0707 15:06:41.268043   254 layer_factory.hpp:172] Creating layer 'fc7' of type 'InnerProduct'\n",
      "I0707 15:06:41.268060   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:41.268083   254 net.cpp:199] Created Layer fc7 (19)\n",
      "I0707 15:06:41.268139   254 net.cpp:571] fc7 <- fc6\n",
      "I0707 15:06:41.268151   254 net.cpp:541] fc7 -> fc7\n",
      "I0707 15:06:41.566195   254 net.cpp:259] Setting up fc7\n",
      "I0707 15:06:41.566243   254 net.cpp:266] TEST Top shape for layer 19 'fc7' 1 4096 (4096)\n",
      "I0707 15:06:41.566264   254 layer_factory.hpp:172] Creating layer 'relu7' of type 'ReLU'\n",
      "I0707 15:06:41.566280   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:41.566298   254 net.cpp:199] Created Layer relu7 (20)\n",
      "I0707 15:06:41.566310   254 net.cpp:571] relu7 <- fc7\n",
      "I0707 15:06:41.566326   254 net.cpp:526] relu7 -> fc7 (in-place)\n",
      "I0707 15:06:41.566347   254 net.cpp:259] Setting up relu7\n",
      "I0707 15:06:41.566359   254 net.cpp:266] TEST Top shape for layer 20 'relu7' 1 4096 (4096)\n",
      "I0707 15:06:41.566372   254 layer_factory.hpp:172] Creating layer 'drop7' of type 'Dropout'\n",
      "I0707 15:06:41.566383   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:41.566401   254 net.cpp:199] Created Layer drop7 (21)\n",
      "I0707 15:06:41.566412   254 net.cpp:571] drop7 <- fc7\n",
      "I0707 15:06:41.566421   254 net.cpp:526] drop7 -> fc7 (in-place)\n",
      "I0707 15:06:41.600190   254 net.cpp:259] Setting up drop7\n",
      "I0707 15:06:41.600229   254 net.cpp:266] TEST Top shape for layer 21 'drop7' 1 4096 (4096)\n",
      "I0707 15:06:41.600242   254 layer_factory.hpp:172] Creating layer 'fc8' of type 'InnerProduct'\n",
      "I0707 15:06:41.600257   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:41.600275   254 net.cpp:199] Created Layer fc8 (22)\n",
      "I0707 15:06:41.600287   254 net.cpp:571] fc8 <- fc7\n",
      "I0707 15:06:41.600297   254 net.cpp:541] fc8 -> fc8\n",
      "I0707 15:06:41.600540   254 net.cpp:259] Setting up fc8\n",
      "I0707 15:06:41.600556   254 net.cpp:266] TEST Top shape for layer 22 'fc8' 1 2 (2)\n",
      "I0707 15:06:41.600569   254 layer_factory.hpp:172] Creating layer 'softmax' of type 'Softmax'\n",
      "I0707 15:06:41.600580   254 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0707 15:06:41.600602   254 net.cpp:199] Created Layer softmax (23)\n",
      "I0707 15:06:41.600613   254 net.cpp:571] softmax <- fc8\n",
      "I0707 15:06:41.600625   254 net.cpp:541] softmax -> softmax\n",
      "I0707 15:06:41.600706   254 net.cpp:259] Setting up softmax\n",
      "I0707 15:06:41.600723   254 net.cpp:266] TEST Top shape for layer 23 'softmax' 1 2 (2)\n",
      "I0707 15:06:41.600733   254 net.cpp:337] softmax does not need backward computation.\n",
      "I0707 15:06:41.600744   254 net.cpp:337] fc8 does not need backward computation.\n",
      "I0707 15:06:41.600750   254 net.cpp:337] drop7 does not need backward computation.\n",
      "I0707 15:06:41.600756   254 net.cpp:337] relu7 does not need backward computation.\n",
      "I0707 15:06:41.600762   254 net.cpp:337] fc7 does not need backward computation.\n",
      "I0707 15:06:41.600769   254 net.cpp:337] drop6 does not need backward computation.\n",
      "I0707 15:06:41.600777   254 net.cpp:337] relu6 does not need backward computation.\n",
      "I0707 15:06:41.600783   254 net.cpp:337] fc6 does not need backward computation.\n",
      "I0707 15:06:41.600795   254 net.cpp:337] pool5 does not need backward computation.\n",
      "I0707 15:06:41.600802   254 net.cpp:337] relu5 does not need backward computation.\n",
      "I0707 15:06:41.600807   254 net.cpp:337] conv5 does not need backward computation.\n",
      "I0707 15:06:41.600819   254 net.cpp:337] relu4 does not need backward computation.\n",
      "I0707 15:06:41.600824   254 net.cpp:337] conv4 does not need backward computation.\n",
      "I0707 15:06:41.600831   254 net.cpp:337] relu3 does not need backward computation.\n",
      "I0707 15:06:41.600841   254 net.cpp:337] conv3 does not need backward computation.\n",
      "I0707 15:06:41.600848   254 net.cpp:337] pool2 does not need backward computation.\n",
      "I0707 15:06:41.600858   254 net.cpp:337] norm2 does not need backward computation.\n",
      "I0707 15:06:41.600864   254 net.cpp:337] relu2 does not need backward computation.\n",
      "I0707 15:06:41.600874   254 net.cpp:337] conv2 does not need backward computation.\n",
      "I0707 15:06:41.600881   254 net.cpp:337] pool1 does not need backward computation.\n",
      "I0707 15:06:41.600919   254 net.cpp:337] norm1 does not need backward computation.\n",
      "I0707 15:06:41.600931   254 net.cpp:337] relu1 does not need backward computation.\n",
      "I0707 15:06:41.600942   254 net.cpp:337] conv1 does not need backward computation.\n",
      "I0707 15:06:41.600955   254 net.cpp:337] input does not need backward computation.\n",
      "I0707 15:06:41.600966   254 net.cpp:379] This network produces output softmax\n",
      "I0707 15:06:41.600993   254 net.cpp:402] Top memory (TEST) required for data: 8315264 diff: 8315264\n",
      "I0707 15:06:41.601004   254 net.cpp:405] Bottom memory (TEST) required for data: 8315256 diff: 8315256\n",
      "I0707 15:06:41.601016   254 net.cpp:408] Shared (in-place) memory (TEST) by data: 2665856 diff: 2665856\n",
      "I0707 15:06:41.601027   254 net.cpp:411] Parameters memory (TEST) required for data: 227505672 diff: 227505672\n",
      "I0707 15:06:41.601032   254 net.cpp:414] Parameters shared memory (TEST) by data: 0 diff: 0\n",
      "I0707 15:06:41.601039   254 net.cpp:420] Network initialization done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0707 15:06:41.705965   254 net.cpp:1129] Ignoring source layer train-data\n",
      "I0707 15:06:41.706001   254 net.cpp:1137] Copying source layer conv1 Type:Convolution #blobs=2\n",
      "I0707 15:06:41.706100   254 net.cpp:1137] Copying source layer relu1 Type:ReLU #blobs=0\n",
      "I0707 15:06:41.706115   254 net.cpp:1137] Copying source layer norm1 Type:LRN #blobs=0\n",
      "I0707 15:06:41.706120   254 net.cpp:1137] Copying source layer pool1 Type:Pooling #blobs=0\n",
      "I0707 15:06:41.706133   254 net.cpp:1137] Copying source layer conv2 Type:Convolution #blobs=2\n",
      "I0707 15:06:41.706317   254 net.cpp:1137] Copying source layer relu2 Type:ReLU #blobs=0\n",
      "I0707 15:06:41.706336   254 net.cpp:1137] Copying source layer norm2 Type:LRN #blobs=0\n",
      "I0707 15:06:41.706344   254 net.cpp:1137] Copying source layer pool2 Type:Pooling #blobs=0\n",
      "I0707 15:06:41.706357   254 net.cpp:1137] Copying source layer conv3 Type:Convolution #blobs=2\n",
      "I0707 15:06:41.706802   254 net.cpp:1137] Copying source layer relu3 Type:ReLU #blobs=0\n",
      "I0707 15:06:41.706820   254 net.cpp:1137] Copying source layer conv4 Type:Convolution #blobs=2\n",
      "I0707 15:06:41.707175   254 net.cpp:1137] Copying source layer relu4 Type:ReLU #blobs=0\n",
      "I0707 15:06:41.707193   254 net.cpp:1137] Copying source layer conv5 Type:Convolution #blobs=2\n",
      "I0707 15:06:41.707455   254 net.cpp:1137] Copying source layer relu5 Type:ReLU #blobs=0\n",
      "I0707 15:06:41.707474   254 net.cpp:1137] Copying source layer pool5 Type:Pooling #blobs=0\n",
      "I0707 15:06:41.707486   254 net.cpp:1137] Copying source layer fc6 Type:InnerProduct #blobs=2\n",
      "I0707 15:06:41.724325   254 net.cpp:1137] Copying source layer relu6 Type:ReLU #blobs=0\n",
      "I0707 15:06:41.724357   254 net.cpp:1137] Copying source layer drop6 Type:Dropout #blobs=0\n",
      "I0707 15:06:41.724364   254 net.cpp:1137] Copying source layer fc7 Type:InnerProduct #blobs=2\n",
      "I0707 15:06:41.731776   254 net.cpp:1137] Copying source layer relu7 Type:ReLU #blobs=0\n",
      "I0707 15:06:41.731809   254 net.cpp:1137] Copying source layer drop7 Type:Dropout #blobs=0\n",
      "I0707 15:06:41.731815   254 net.cpp:1137] Copying source layer fc8 Type:InnerProduct #blobs=2\n",
      "I0707 15:06:41.731855   254 net.cpp:1129] Ignoring source layer loss\n",
      "not whale\n"
     ]
    }
   ],
   "source": [
    "!python submission.py '/dli/data/whale/data/train/not_face/w_1.jpg'  #This should return \"not whale\" at the very bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessment1-ja.ipynb  Assessment1.ipynb    Next Steps.ipynb  Project.ipynb\r\n",
      "Assessment1-kr.ipynb  Next Steps-ja.ipynb  Project-ja.ipynb  images\r\n",
      "Assessment1-zh.ipynb  Next Steps-zh.ipynb  Project-zh.ipynb  submission.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Assessment1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
